package jitesh.spark
import org.apache.spark.SparkContext
import org.apache.spark.SparkConf
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.Seconds
import twitter4j.conf.ConfigurationBuilder
import twitter4j.auth.OAuthAuthorization
import twitter4j.Status
import org.apache.spark.streaming.twitter.TwitterUtils
import java.sql.{ Connection, DriverManager }
import org.apache.spark.sql
import org.apache.spark.sql.functions.lit
import org.apache.spark.sql.Column
import org.apache.spark.streaming.{ Seconds, StreamingContext, Time }
import org.apache.spark.sql.SparkSession
//import org.apache.spark.sql.SQLContext.implicits._

import org.apache.spark.rdd.RDD
import org.apache.spark.sql._
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.streaming.StreamingContext._

object tweet {

  val sc = new SparkConf().setAppName("streamtwitter").set("spark.driver.allowMultipleContexts", "true");
  val conf = new SparkContext(sc)
  val sqlContext = new org.apache.spark.sql.SQLContext(conf)
  //val sqlContext= new org.apache.spark.sql.SQLContext(sc)

  val url = "jdbc:mysql://localhost:3306/demo?user=demo&password=demo"
  val driver = "com.mysql.cj.jdbc.Driver"
  val username = "demo"
  val password = "demo"
  var connection: Connection = _

  def main(args: Array[String]) {
    if (args.length < 4) {
      System.err.println("Usage: TwitterData <ConsumerKey><ConsumerSecret><accessToken><accessTokenSecret>" +
        "[<filters>]")
      System.exit(1)
    }
    val appName = "TwitterData"
    val conf = new SparkConf()
    conf.setAppName(appName).setMaster("local[2]")
    val ssc = new StreamingContext(conf, Seconds(5))
    val Array(consumerKey, consumerSecret, accessToken, accessTokenSecret) = args.take(4)
    val filters = args.takeRight(args.length - 4)
    val cb = new ConfigurationBuilder
    cb.setDebugEnabled(true).setOAuthConsumerKey(consumerKey)
      .setOAuthConsumerSecret(consumerSecret)
      .setOAuthAccessToken(accessToken)
      .setOAuthAccessTokenSecret(accessTokenSecret)
    val auth = new OAuthAuthorization(cb.build)
    val tweets = TwitterUtils.createStream(ssc, Some(auth), filters)

    /*
    val data = tweets.map {status =>
      val places = status.getPlace
      val id = status.getUser.getId
      val date = status.getUser.getCreatedAt.toString()
      val user = status.getUser.getName()
      val place = places.getCountry()

      (id,date,user,place)
      }
    data.foreachRDD{rdd =>
      //import spark.implicits._
      import sqlContext.implicits._
     val tweets = rdd.toDF("id","date","user","place").show()


    }
  */

    val tweet_print = tweets.map(tuple => "%s,%s,%s,%s,%s".format(
      tuple.getId,
      tuple.getCreatedAt, tuple.getSource, tuple.getText.toLowerCase.replaceAll(",", " "), tuple.getGeoLocation)).print
    //val words: tweets[String] = ...
      case class Tweet(createdAt:Long, text:String)
      
    tweets.map(status=>
          Tweet(status.getCreatedAt().getTime()/1000, status.getText())).foreachRDD { rdd =>

      // Get the singleton instance of SparkSession
      val spark = SparkSessionSingleton.getInstance(rdd.sparkContext.getConf)
      val sqlContext = SQLContext.getOrCreate(rdd.sparkContext)
      //import org.apache.spark.sql.SQLContext.implicits._
      import sqlContext._
      import sqlContext.implicits._
      // Convert RDD[String] to DataFrame
      val wordsDataFrame = rdd.toDF.registerTempTable("word")
      // Create a temporary view
      //wordsDataFrame.createOrReplaceTempView("words")

      // Do word count on DataFrame using SQL and print it
      val wordCountsDataFrame =
        spark.sql("select word, count(*) as total from words group by word")
      wordCountsDataFrame.show()
    }

    /*
    tweets.foreachRDD { rdd =>
      if (!rdd.isEmpty) {
        rdd.foreachPartition {it =>

            if (!it.isEmpty) {

              val conn = DriverManager.getConnection(url, username, password)

            val text = rdd.getText()
            val id = rdd.map(it => it.getId)
            val created_at = rdd.map(it => it.getCreatedAt)
            val source = rdd.map(it => it.getSource)
            val geo = rdd.map(it => it.getGeoLocation)

          /*val text = it.map(tuple => tuple.getText)
          val id = it.map(tuple => tuple.getId)
          val created_at = it.map(tuple => tuple.getCreatedAt)
          val source = it.map(tuple => tuple.getSource)
          val geo = it.map(tuple => tuple.getGeoLocation)
          //val showtext =text.toString().p
          */
          // while ((it.hasNext))
           //{
              val del = conn.prepareStatement("INSERT INTO tweetstable (ID,CreatedAt,Source,Text,GeoLocation) VALUES (?,?,?,?,?)")
               //val del = conn.prepareStatement("INSERT INTO tweetstable (ID,CreatedAt,Source,Text,GeoLocation) VALUES (text,id,created_at,source,geo)")

                del.setString(1, it.map(it => it.getText).toString())
                del.setString(2, it.map(it => it.getId).toString())
                del.setString(3, it.map(it => it.getSource).toString())
                del.setString(4, it.map(it => it.getSource).toString())
                del.setString(5, it.map(it => it.getGeoLocation).toString())


               /*
              for (tuple <- it) {
                del.setLong(1, tuple.getId)
                del.setString(2, tuple.getCreatedAt.toString)
                del.setString(3, tuple.getSource)
                del.setString(4, tuple.getText)
                del.setString(5, tuple.getGeoLocation.toString)
                // } */
                del.executeUpdate()
                //del.close()
                 //conn.close()

           }



    }
      }

      }
*/
    ssc.start()
    ssc.awaitTermination()
  }
}

/** Case class for converting RDD to DataFrame */
case class Record(word: String)

/** Lazily instantiated singleton instance of SparkSession */
object SparkSessionSingleton {

  @transient private var instance: SparkSession = _

  def getInstance(sparkConf: SparkConf): SparkSession = {
    if (instance == null) {
      instance = SparkSession
        .builder
        .config(sparkConf)
        .getOrCreate()
    }
    instance
  }
}